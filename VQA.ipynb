{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCIbLbpEfbYk"
      },
      "source": [
        "#Install if not available \n",
        "#If installing on colab, restart runtime \n",
        "#before running the rest of the notebook\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install keras-self-attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48W3w6ycamA"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ6XNNcldSFk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import spacy\n",
        "import scipy.io\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
        "from keras import Input\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import concatenate\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import model_from_json, Model\n",
        "from keras.utils import plot_model\n",
        "from collections import defaultdict\n",
        "import operator\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from progressbar import Bar, ETA, Percentage, ProgressBar\n",
        "from itertools import zip_longest\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7FGHYUucamJ"
      },
      "source": [
        "### Reading Preprocessed Files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8obF-IZDcamK"
      },
      "source": [
        "questions = open('/content/drive/Processed Text Files/Train Files/ques.txt', 'rb').read().decode('utf-8').splitlines()\n",
        "questions_len = open('/content/drive/Processed Text Files/Train Files/ques_len.txt', 'rb').read().decode('utf-8').splitlines()\n",
        "answers = open('/content/drive/Processed Text Files/Train Files/answer.txt','rb').read().decode('utf-8').splitlines()\n",
        "image_id = open('/content/drive/Processed Text Files/Train Files/images_id.txt','rb').read().decode('utf-8').splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPItIvyRcamK",
        "outputId": "a383fd7c-b589-4a93-d393-1ba085e2cfe2"
      },
      "source": [
        "print(questions[0])\n",
        "print(answers[0])\n",
        "print(len(image_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the overall condition of the given image?\n",
            "flooded\n",
            "4511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5GDZOuKcamL"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yRNuPR2bKFy"
      },
      "source": [
        "features1 = open(\"/content/drive/Image Feature Files/VGG/Train Features.txt\",\"rb\")\n",
        "features2 = open(\"/content/drive/Image Feature Files/VGG/Test Features.txt\",\"rb\")\n",
        "features_train = pickle.load(features1)\n",
        "features_test = pickle.load(features2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUw72HNTcGuo",
        "outputId": "8eda903c-7550-4f46-d24b-cd068ccd1a81"
      },
      "source": [
        "print(features_train.shape)\n",
        "print(features_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4096, 1448)\n",
            "(4096, 450)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maYZ_HE4eeRQ",
        "outputId": "f33b9f59-fab0-4b82-9a64-676416809bb9"
      },
      "source": [
        "questions_len, questions, answers, image_id = (list(t) for t in zip(*sorted(zip(questions_len, questions, answers, image_id))))\n",
        "print (len(questions), len(answers),len(image_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4511 4511 4511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hjBovSdeyNf"
      },
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(answers)\n",
        "pickle.dump(le, open('/content/drive/label_encoder_lstm.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T96efJuEcamP"
      },
      "source": [
        "### Defining Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl9GdRJkgQYp"
      },
      "source": [
        "batch_size               =      512\n",
        "img_dim                  =     4096\n",
        "word2vec_dim             =      300\n",
        "num_hidden_nodes_mlp     =     1024\n",
        "num_hidden_nodes_lstm    =      512\n",
        "num_layers_lstm          =        3\n",
        "dropout                  =       0.5\n",
        "activation_mlp           =     'relu'\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR6xxQAQgS0Z"
      },
      "source": [
        "img_ids = open('/content/drive/Processed Text Files/Train Files/images_id.txt','rb').read().decode('utf-8').splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i56IAK3mCMn",
        "outputId": "b7ab423a-f2d1-4589-c264-c79de9ed803a"
      },
      "source": [
        "id_map = dict()\n",
        "i = 0\n",
        "for ids in np.unique(img_ids):\n",
        "  id_map[ids] = i\n",
        "  i+=1\n",
        "print(id_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'10165': 0, '10166': 1, '10168': 2, '10170': 3, '10171': 4, '10172': 5, '10175': 6, '10176': 7, '10179': 8, '10180': 9, '10181': 10, '10182': 11, '10184': 12, '10300': 13, '10566': 14, '10687': 15, '10806': 16, '10810': 17, '10811': 18, '10817': 19, '10818': 20, '10819': 21, '10820': 22, '10821': 23, '10825': 24, '10826': 25, '10827': 26, '10828': 27, '10834': 28, '10835': 29, '10836': 30, '10837': 31, '10840': 32, '10841': 33, '11723': 34, '6279': 35, '6287': 36, '6332': 37, '6334': 38, '6335': 39, '6338': 40, '6340': 41, '6341': 42, '6344': 43, '6346': 44, '6347': 45, '6348': 46, '6350': 47, '6351': 48, '6352': 49, '6354': 50, '6358': 51, '6359': 52, '6360': 53, '6361': 54, '6363': 55, '6364': 56, '6365': 57, '6367': 58, '6368': 59, '6369': 60, '6370': 61, '6372': 62, '6373': 63, '6374': 64, '6375': 65, '6376': 66, '6379': 67, '6381': 68, '6384': 69, '6385': 70, '6386': 71, '6387': 72, '6388': 73, '6393': 74, '6395': 75, '6397': 76, '6398': 77, '6399': 78, '6402': 79, '6403': 80, '6404': 81, '6407': 82, '6408': 83, '6409': 84, '6411': 85, '6413': 86, '6414': 87, '6415': 88, '6416': 89, '6421': 90, '6422': 91, '6424': 92, '6426': 93, '6427': 94, '6430': 95, '6431': 96, '6433': 97, '6435': 98, '6437': 99, '6438': 100, '6442': 101, '6443': 102, '6444': 103, '6446': 104, '6447': 105, '6450': 106, '6451': 107, '6453': 108, '6454': 109, '6455': 110, '6456': 111, '6457': 112, '6458': 113, '6459': 114, '6460': 115, '6461': 116, '6462': 117, '6463': 118, '6464': 119, '6465': 120, '6466': 121, '6469': 122, '6477': 123, '6478': 124, '6481': 125, '6484': 126, '6486': 127, '6492': 128, '6496': 129, '6498': 130, '6499': 131, '6501': 132, '6504': 133, '6505': 134, '6506': 135, '6509': 136, '6510': 137, '6512': 138, '6513': 139, '6516': 140, '6520': 141, '6523': 142, '6525': 143, '6526': 144, '6527': 145, '6528': 146, '6530': 147, '6531': 148, '6533': 149, '6534': 150, '6535': 151, '6537': 152, '6538': 153, '6540': 154, '6541': 155, '6542': 156, '6543': 157, '6544': 158, '6546': 159, '6548': 160, '6549': 161, '6552': 162, '6555': 163, '6556': 164, '6559': 165, '6560': 166, '6563': 167, '6564': 168, '6565': 169, '6566': 170, '6569': 171, '6573': 172, '6574': 173, '6580': 174, '6584': 175, '6585': 176, '6586': 177, '6588': 178, '6589': 179, '6591': 180, '6592': 181, '6595': 182, '6597': 183, '6600': 184, '6601': 185, '6603': 186, '6608': 187, '6610': 188, '6613': 189, '6614': 190, '6615': 191, '6616': 192, '6617': 193, '6618': 194, '6620': 195, '6622': 196, '6623': 197, '6625': 198, '6627': 199, '6628': 200, '6633': 201, '6635': 202, '6639': 203, '6640': 204, '6643': 205, '6644': 206, '6645': 207, '6646': 208, '6647': 209, '6648': 210, '6649': 211, '6651': 212, '6652': 213, '6654': 214, '6655': 215, '6656': 216, '6657': 217, '6658': 218, '6660': 219, '6662': 220, '6664': 221, '6666': 222, '6667': 223, '6668': 224, '6670': 225, '6673': 226, '6675': 227, '6676': 228, '6678': 229, '6681': 230, '6682': 231, '6685': 232, '6686': 233, '6687': 234, '6689': 235, '6692': 236, '6693': 237, '6701': 238, '6703': 239, '6704': 240, '6706': 241, '6707': 242, '6708': 243, '6710': 244, '6711': 245, '6712': 246, '6713': 247, '6714': 248, '6715': 249, '6716': 250, '6719': 251, '6727': 252, '6728': 253, '6731': 254, '6732': 255, '6734': 256, '6735': 257, '6736': 258, '6737': 259, '6738': 260, '6739': 261, '6742': 262, '6743': 263, '6745': 264, '6746': 265, '6747': 266, '6748': 267, '6749': 268, '6750': 269, '6754': 270, '6755': 271, '6756': 272, '6760': 273, '6762': 274, '6763': 275, '6764': 276, '6770': 277, '6772': 278, '6773': 279, '6776': 280, '6779': 281, '6783': 282, '6787': 283, '6788': 284, '6789': 285, '6790': 286, '6792': 287, '6793': 288, '6797': 289, '6798': 290, '6799': 291, '6800': 292, '6802': 293, '6803': 294, '6804': 295, '6805': 296, '6806': 297, '6807': 298, '6808': 299, '6810': 300, '6812': 301, '6813': 302, '6814': 303, '6818': 304, '6819': 305, '6820': 306, '6824': 307, '6826': 308, '6829': 309, '6830': 310, '6832': 311, '6833': 312, '6836': 313, '6838': 314, '6839': 315, '6842': 316, '6843': 317, '6846': 318, '6849': 319, '6851': 320, '6852': 321, '6853': 322, '6854': 323, '6855': 324, '6857': 325, '6858': 326, '6859': 327, '6860': 328, '6890': 329, '6891': 330, '6899': 331, '6900': 332, '6901': 333, '6902': 334, '6904': 335, '6908': 336, '6909': 337, '6911': 338, '6913': 339, '6914': 340, '6918': 341, '6919': 342, '6922': 343, '6924': 344, '6926': 345, '6928': 346, '6929': 347, '6930': 348, '6931': 349, '6933': 350, '6934': 351, '6935': 352, '6936': 353, '6938': 354, '6939': 355, '6940': 356, '6941': 357, '6943': 358, '6945': 359, '6946': 360, '6947': 361, '6953': 362, '6955': 363, '6956': 364, '6957': 365, '6960': 366, '6964': 367, '6965': 368, '6966': 369, '6967': 370, '6968': 371, '6969': 372, '6970': 373, '6971': 374, '6972': 375, '6974': 376, '6976': 377, '6978': 378, '6981': 379, '6982': 380, '6985': 381, '6986': 382, '6992': 383, '6993': 384, '6994': 385, '6995': 386, '6996': 387, '6997': 388, '6998': 389, '6999': 390, '7000': 391, '7003': 392, '7006': 393, '7007': 394, '7008': 395, '7009': 396, '7010': 397, '7011': 398, '7014': 399, '7015': 400, '7017': 401, '7018': 402, '7021': 403, '7022': 404, '7024': 405, '7025': 406, '7026': 407, '7027': 408, '7028': 409, '7033': 410, '7036': 411, '7037': 412, '7038': 413, '7039': 414, '7043': 415, '7044': 416, '7045': 417, '7046': 418, '7047': 419, '7049': 420, '7050': 421, '7051': 422, '7053': 423, '7055': 424, '7059': 425, '7061': 426, '7064': 427, '7065': 428, '7070': 429, '7071': 430, '7072': 431, '7073': 432, '7075': 433, '7077': 434, '7078': 435, '7079': 436, '7081': 437, '7082': 438, '7083': 439, '7086': 440, '7087': 441, '7088': 442, '7090': 443, '7091': 444, '7094': 445, '7096': 446, '7097': 447, '7098': 448, '7100': 449, '7102': 450, '7103': 451, '7107': 452, '7108': 453, '7109': 454, '7110': 455, '7115': 456, '7118': 457, '7121': 458, '7122': 459, '7123': 460, '7124': 461, '7125': 462, '7127': 463, '7128': 464, '7130': 465, '7131': 466, '7132': 467, '7136': 468, '7142': 469, '7144': 470, '7146': 471, '7147': 472, '7151': 473, '7156': 474, '7157': 475, '7160': 476, '7161': 477, '7162': 478, '7163': 479, '7164': 480, '7165': 481, '7167': 482, '7168': 483, '7170': 484, '7171': 485, '7174': 486, '7175': 487, '7176': 488, '7177': 489, '7178': 490, '7179': 491, '7181': 492, '7182': 493, '7183': 494, '7184': 495, '7185': 496, '7186': 497, '7187': 498, '7188': 499, '7189': 500, '7192': 501, '7193': 502, '7194': 503, '7195': 504, '7199': 505, '7201': 506, '7202': 507, '7205': 508, '7209': 509, '7210': 510, '7211': 511, '7212': 512, '7214': 513, '7219': 514, '7221': 515, '7222': 516, '7225': 517, '7226': 518, '7227': 519, '7229': 520, '7230': 521, '7235': 522, '7236': 523, '7238': 524, '7239': 525, '7240': 526, '7242': 527, '7243': 528, '7244': 529, '7245': 530, '7247': 531, '7251': 532, '7252': 533, '7256': 534, '7257': 535, '7258': 536, '7261': 537, '7263': 538, '7264': 539, '7265': 540, '7266': 541, '7267': 542, '7270': 543, '7271': 544, '7272': 545, '7273': 546, '7278': 547, '7282': 548, '7285': 549, '7287': 550, '7288': 551, '7289': 552, '7290': 553, '7292': 554, '7293': 555, '7298': 556, '7299': 557, '7304': 558, '7305': 559, '7306': 560, '7310': 561, '7311': 562, '7312': 563, '7313': 564, '7314': 565, '7316': 566, '7317': 567, '7318': 568, '7319': 569, '7321': 570, '7322': 571, '7324': 572, '7325': 573, '7328': 574, '7330': 575, '7331': 576, '7332': 577, '7335': 578, '7336': 579, '7340': 580, '7344': 581, '7345': 582, '7353': 583, '7354': 584, '7356': 585, '7357': 586, '7362': 587, '7363': 588, '7364': 589, '7366': 590, '7372': 591, '7374': 592, '7375': 593, '7377': 594, '7405': 595, '7410': 596, '7411': 597, '7412': 598, '7414': 599, '7418': 600, '7419': 601, '7422': 602, '7426': 603, '7427': 604, '7428': 605, '7429': 606, '7430': 607, '7434': 608, '7436': 609, '7438': 610, '7439': 611, '7440': 612, '7443': 613, '7445': 614, '7448': 615, '7449': 616, '7451': 617, '7452': 618, '7453': 619, '7454': 620, '7458': 621, '7459': 622, '7460': 623, '7462': 624, '7463': 625, '7465': 626, '7466': 627, '7467': 628, '7469': 629, '7472': 630, '7473': 631, '7474': 632, '7475': 633, '7477': 634, '7481': 635, '7485': 636, '7487': 637, '7488': 638, '7489': 639, '7520': 640, '7521': 641, '7522': 642, '7523': 643, '7524': 644, '7525': 645, '7526': 646, '7542': 647, '7545': 648, '7546': 649, '7547': 650, '7548': 651, '7552': 652, '7556': 653, '7557': 654, '7559': 655, '7560': 656, '7566': 657, '7567': 658, '7568': 659, '7571': 660, '7573': 661, '7574': 662, '7575': 663, '7579': 664, '7580': 665, '7582': 666, '7585': 667, '7587': 668, '7589': 669, '7594': 670, '7596': 671, '7597': 672, '7600': 673, '7601': 674, '7602': 675, '7603': 676, '7606': 677, '7609': 678, '7612': 679, '7614': 680, '7641': 681, '7643': 682, '7645': 683, '7646': 684, '7648': 685, '7650': 686, '7653': 687, '7654': 688, '7655': 689, '7659': 690, '7661': 691, '7662': 692, '7664': 693, '7665': 694, '7671': 695, '7672': 696, '7673': 697, '7674': 698, '7675': 699, '7677': 700, '7678': 701, '7680': 702, '7681': 703, '7682': 704, '7685': 705, '7686': 706, '7688': 707, '7691': 708, '7692': 709, '7693': 710, '7696': 711, '7697': 712, '7700': 713, '7702': 714, '7703': 715, '7704': 716, '7705': 717, '7706': 718, '7707': 719, '7709': 720, '7711': 721, '7712': 722, '7713': 723, '7714': 724, '7715': 725, '7717': 726, '7718': 727, '7719': 728, '7720': 729, '7721': 730, '7722': 731, '7724': 732, '7725': 733, '7726': 734, '7727': 735, '7728': 736, '7729': 737, '7731': 738, '7735': 739, '7736': 740, '7737': 741, '7739': 742, '7746': 743, '7748': 744, '7749': 745, '7750': 746, '7751': 747, '7752': 748, '7753': 749, '7754': 750, '7756': 751, '7760': 752, '7761': 753, '7764': 754, '7765': 755, '7766': 756, '7767': 757, '7768': 758, '7770': 759, '7774': 760, '7775': 761, '7776': 762, '7777': 763, '7778': 764, '7782': 765, '7783': 766, '7784': 767, '7786': 768, '7787': 769, '7788': 770, '7789': 771, '7791': 772, '7792': 773, '7795': 774, '7796': 775, '7798': 776, '7800': 777, '7801': 778, '7802': 779, '7806': 780, '7807': 781, '7808': 782, '7809': 783, '7810': 784, '7814': 785, '7815': 786, '7816': 787, '7818': 788, '7820': 789, '7825': 790, '7827': 791, '7828': 792, '7829': 793, '7830': 794, '7831': 795, '7832': 796, '7833': 797, '7834': 798, '7837': 799, '7841': 800, '7842': 801, '7843': 802, '7844': 803, '7845': 804, '7847': 805, '7848': 806, '7849': 807, '7852': 808, '7855': 809, '7857': 810, '7858': 811, '7859': 812, '7861': 813, '7862': 814, '7864': 815, '7865': 816, '7867': 817, '7868': 818, '7869': 819, '7871': 820, '7872': 821, '7873': 822, '7875': 823, '7878': 824, '7879': 825, '7882': 826, '7883': 827, '7884': 828, '7890': 829, '7892': 830, '7893': 831, '7894': 832, '7895': 833, '7896': 834, '7901': 835, '7902': 836, '7905': 837, '7907': 838, '7908': 839, '7909': 840, '7910': 841, '7914': 842, '7916': 843, '7917': 844, '7918': 845, '7919': 846, '7920': 847, '7922': 848, '7923': 849, '7924': 850, '7925': 851, '7929': 852, '7931': 853, '7932': 854, '7933': 855, '7934': 856, '7939': 857, '7944': 858, '7945': 859, '7947': 860, '7948': 861, '7949': 862, '7950': 863, '7951': 864, '7952': 865, '7953': 866, '7954': 867, '7958': 868, '7959': 869, '7962': 870, '7963': 871, '7965': 872, '7966': 873, '7967': 874, '7971': 875, '7972': 876, '7973': 877, '7974': 878, '7975': 879, '7977': 880, '7979': 881, '7980': 882, '7981': 883, '7982': 884, '7987': 885, '7989': 886, '7996': 887, '7998': 888, '7999': 889, '8000': 890, '8001': 891, '8002': 892, '8003': 893, '8005': 894, '8008': 895, '8009': 896, '8010': 897, '8011': 898, '8013': 899, '8014': 900, '8015': 901, '8016': 902, '8020': 903, '8024': 904, '8029': 905, '8030': 906, '8032': 907, '8033': 908, '8034': 909, '8037': 910, '8038': 911, '8039': 912, '8042': 913, '8043': 914, '8044': 915, '8045': 916, '8046': 917, '8047': 918, '8049': 919, '8051': 920, '8052': 921, '8053': 922, '8055': 923, '8056': 924, '8057': 925, '8059': 926, '8060': 927, '8061': 928, '8063': 929, '8065': 930, '8068': 931, '8070': 932, '8072': 933, '8074': 934, '8075': 935, '8077': 936, '8080': 937, '8081': 938, '8082': 939, '8083': 940, '8085': 941, '8086': 942, '8087': 943, '8089': 944, '8090': 945, '8091': 946, '8093': 947, '8094': 948, '8095': 949, '8096': 950, '8098': 951, '8099': 952, '8101': 953, '8102': 954, '8106': 955, '8107': 956, '8108': 957, '8109': 958, '8110': 959, '8116': 960, '8117': 961, '8118': 962, '8119': 963, '8121': 964, '8122': 965, '8129': 966, '8130': 967, '8131': 968, '8132': 969, '8133': 970, '8134': 971, '8135': 972, '8137': 973, '8138': 974, '8139': 975, '8141': 976, '8142': 977, '8143': 978, '8145': 979, '8146': 980, '8148': 981, '8149': 982, '8150': 983, '8151': 984, '8152': 985, '8153': 986, '8157': 987, '8158': 988, '8159': 989, '8161': 990, '8164': 991, '8165': 992, '8166': 993, '8167': 994, '8168': 995, '8172': 996, '8173': 997, '8174': 998, '8175': 999, '8176': 1000, '8177': 1001, '8182': 1002, '8184': 1003, '8185': 1004, '8190': 1005, '8192': 1006, '8193': 1007, '8196': 1008, '8197': 1009, '8200': 1010, '8202': 1011, '8205': 1012, '8206': 1013, '8207': 1014, '8208': 1015, '8210': 1016, '8212': 1017, '8216': 1018, '8217': 1019, '8221': 1020, '8222': 1021, '8223': 1022, '8224': 1023, '8225': 1024, '8226': 1025, '8227': 1026, '8228': 1027, '8229': 1028, '8230': 1029, '8231': 1030, '8232': 1031, '8234': 1032, '8235': 1033, '8236': 1034, '8240': 1035, '8241': 1036, '8242': 1037, '8243': 1038, '8244': 1039, '8245': 1040, '8247': 1041, '8248': 1042, '8249': 1043, '8251': 1044, '8252': 1045, '8253': 1046, '8255': 1047, '8256': 1048, '8260': 1049, '8261': 1050, '8264': 1051, '8265': 1052, '8267': 1053, '8269': 1054, '8271': 1055, '8272': 1056, '8274': 1057, '8275': 1058, '8276': 1059, '8277': 1060, '8278': 1061, '8280': 1062, '8281': 1063, '8282': 1064, '8284': 1065, '8286': 1066, '8288': 1067, '8294': 1068, '8297': 1069, '8298': 1070, '8299': 1071, '8304': 1072, '8305': 1073, '8306': 1074, '8311': 1075, '8312': 1076, '8313': 1077, '8315': 1078, '8317': 1079, '8318': 1080, '8319': 1081, '8320': 1082, '8321': 1083, '8322': 1084, '8326': 1085, '8330': 1086, '8334': 1087, '8335': 1088, '8336': 1089, '8342': 1090, '8344': 1091, '8345': 1092, '8347': 1093, '8348': 1094, '8350': 1095, '8351': 1096, '8352': 1097, '8354': 1098, '8355': 1099, '8357': 1100, '8359': 1101, '8361': 1102, '8366': 1103, '8368': 1104, '8369': 1105, '8370': 1106, '8371': 1107, '8374': 1108, '8375': 1109, '8379': 1110, '8380': 1111, '8381': 1112, '8382': 1113, '8385': 1114, '8386': 1115, '8387': 1116, '8388': 1117, '8389': 1118, '8390': 1119, '8392': 1120, '8393': 1121, '8394': 1122, '8398': 1123, '8400': 1124, '8403': 1125, '8404': 1126, '8405': 1127, '8406': 1128, '8407': 1129, '8408': 1130, '8409': 1131, '8410': 1132, '8411': 1133, '8412': 1134, '8413': 1135, '8415': 1136, '8416': 1137, '8417': 1138, '8418': 1139, '8419': 1140, '8420': 1141, '8422': 1142, '8423': 1143, '8424': 1144, '8425': 1145, '8426': 1146, '8427': 1147, '8429': 1148, '8430': 1149, '8431': 1150, '8432': 1151, '8433': 1152, '8436': 1153, '8437': 1154, '8438': 1155, '8442': 1156, '8443': 1157, '8445': 1158, '8463': 1159, '8464': 1160, '8465': 1161, '8469': 1162, '8470': 1163, '8472': 1164, '8473': 1165, '8475': 1166, '8477': 1167, '8479': 1168, '8480': 1169, '8481': 1170, '8482': 1171, '8483': 1172, '8484': 1173, '8485': 1174, '8487': 1175, '8488': 1176, '8490': 1177, '8491': 1178, '8492': 1179, '8493': 1180, '8496': 1181, '8497': 1182, '8498': 1183, '8500': 1184, '8501': 1185, '8502': 1186, '8504': 1187, '8506': 1188, '8507': 1189, '8508': 1190, '8510': 1191, '8512': 1192, '8515': 1193, '8516': 1194, '8517': 1195, '8520': 1196, '8521': 1197, '8522': 1198, '8524': 1199, '8526': 1200, '8527': 1201, '8528': 1202, '8529': 1203, '8531': 1204, '8535': 1205, '8536': 1206, '8538': 1207, '8539': 1208, '8540': 1209, '8545': 1210, '8546': 1211, '8547': 1212, '8548': 1213, '8550': 1214, '8551': 1215, '8552': 1216, '8553': 1217, '8555': 1218, '8556': 1219, '8560': 1220, '8586': 1221, '8587': 1222, '8588': 1223, '8589': 1224, '8590': 1225, '8593': 1226, '8594': 1227, '8597': 1228, '8598': 1229, '8599': 1230, '8600': 1231, '8603': 1232, '8604': 1233, '8605': 1234, '8615': 1235, '8638': 1236, '8658': 1237, '8659': 1238, '8770': 1239, '8774': 1240, '8775': 1241, '8777': 1242, '8783': 1243, '8785': 1244, '8786': 1245, '8791': 1246, '8794': 1247, '8796': 1248, '8798': 1249, '8799': 1250, '8800': 1251, '8801': 1252, '8802': 1253, '8803': 1254, '8804': 1255, '8805': 1256, '8807': 1257, '8809': 1258, '8816': 1259, '8817': 1260, '8818': 1261, '8820': 1262, '8821': 1263, '8822': 1264, '8823': 1265, '8833': 1266, '8859': 1267, '8879': 1268, '8886': 1269, '8895': 1270, '8900': 1271, '8901': 1272, '8904': 1273, '8905': 1274, '8906': 1275, '8907': 1276, '8909': 1277, '8910': 1278, '8911': 1279, '8914': 1280, '8915': 1281, '8916': 1282, '8917': 1283, '8918': 1284, '8921': 1285, '8922': 1286, '8923': 1287, '8924': 1288, '8925': 1289, '8927': 1290, '8928': 1291, '8929': 1292, '8931': 1293, '8932': 1294, '8933': 1295, '8935': 1296, '8938': 1297, '8940': 1298, '8943': 1299, '8947': 1300, '8951': 1301, '8952': 1302, '8953': 1303, '8955': 1304, '8956': 1305, '8957': 1306, '8958': 1307, '8960': 1308, '8962': 1309, '8963': 1310, '8965': 1311, '8966': 1312, '8967': 1313, '8968': 1314, '8969': 1315, '8970': 1316, '8971': 1317, '8973': 1318, '8974': 1319, '8975': 1320, '8977': 1321, '8978': 1322, '8980': 1323, '8982': 1324, '8984': 1325, '8986': 1326, '8987': 1327, '8994': 1328, '8995': 1329, '8996': 1330, '8998': 1331, '9000': 1332, '9003': 1333, '9007': 1334, '9008': 1335, '9010': 1336, '9011': 1337, '9015': 1338, '9017': 1339, '9019': 1340, '9022': 1341, '9023': 1342, '9025': 1343, '9026': 1344, '9027': 1345, '9028': 1346, '9029': 1347, '9030': 1348, '9033': 1349, '9034': 1350, '9036': 1351, '9039': 1352, '9040': 1353, '9043': 1354, '9047': 1355, '9049': 1356, '9050': 1357, '9052': 1358, '9053': 1359, '9059': 1360, '9061': 1361, '9062': 1362, '9063': 1363, '9064': 1364, '9065': 1365, '9066': 1366, '9069': 1367, '9070': 1368, '9071': 1369, '9072': 1370, '9073': 1371, '9074': 1372, '9075': 1373, '9076': 1374, '9077': 1375, '9078': 1376, '9079': 1377, '9080': 1378, '9081': 1379, '9083': 1380, '9084': 1381, '9085': 1382, '9086': 1383, '9087': 1384, '9088': 1385, '9089': 1386, '9090': 1387, '9091': 1388, '9093': 1389, '9094': 1390, '9095': 1391, '9098': 1392, '9099': 1393, '9100': 1394, '9101': 1395, '9103': 1396, '9104': 1397, '9105': 1398, '9106': 1399, '9108': 1400, '9110': 1401, '9115': 1402, '9116': 1403, '9144': 1404, '9162': 1405, '9278': 1406, '9287': 1407, '9343': 1408, '9345': 1409, '9346': 1410, '9349': 1411, '9351': 1412, '9353': 1413, '9365': 1414, '9366': 1415, '9459': 1416, '9511': 1417, '9513': 1418, '9514': 1419, '9538': 1420, '9546': 1421, '9558': 1422, '9718': 1423, '9722': 1424, '9723': 1425, '9724': 1426, '9725': 1427, '9795': 1428, '9796': 1429, '9797': 1430, '9874': 1431, '9875': 1432, '9876': 1433, '9877': 1434, '9879': 1435, '9880': 1436, '9884': 1437, '9885': 1438, '9887': 1439, '9890': 1440, '9892': 1441, '9894': 1442, '9895': 1443, '9896': 1444, '9897': 1445, '9898': 1446, '9925': 1447}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X0vpZfccamP"
      },
      "source": [
        "### Defining network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spGReGzigeCw",
        "outputId": "2e95caf6-a5c4-4e38-b5aa-691f7bf38a3a"
      },
      "source": [
        "image_model = Sequential()\n",
        "image_model.add(Reshape(input_shape = (4096,), target_shape=(4096,)))\n",
        "model1 = Model(inputs = image_model.input, outputs = image_model.output)\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_input (InputLayer)   [(None, 4096)]            0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4096)              0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynH_3KAmggvE",
        "outputId": "e90cd87b-7984-4af0-aed6-6724fb135694"
      },
      "source": [
        "language_model = keras.Sequential()\n",
        "language_model.add(LSTM(units=num_hidden_nodes_lstm, \n",
        "                        return_sequences=True, input_shape=(None, word2vec_dim)))\n",
        "language_model.add(tf.keras.layers.LayerNormalization())\n",
        "language_model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "language_model.add(tf.keras.layers.LayerNormalization())\n",
        "language_model.add(LSTM(units=num_hidden_nodes_lstm, return_sequences=True))\n",
        "language_model.add(tf.keras.layers.LayerNormalization())\n",
        "language_model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "language_model.add(tf.keras.layers.LayerNormalization())\n",
        "language_model.add(LSTM(units=num_hidden_nodes_lstm, return_sequences=False))\n",
        "model2 = tf.keras.Model(language_model.input, language_model.output)\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_input (InputLayer)      [(None, None, 300)]       0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 512)         1665024   \n",
            "_________________________________________________________________\n",
            "layer_normalization (LayerNo (None, None, 512)         1024      \n",
            "_________________________________________________________________\n",
            "seq_self_attention (SeqSelfA (None, None, 512)         32833     \n",
            "_________________________________________________________________\n",
            "layer_normalization_1 (Layer (None, None, 512)         1024      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "layer_normalization_2 (Layer (None, None, 512)         1024      \n",
            "_________________________________________________________________\n",
            "seq_self_attention_1 (SeqSel (None, None, 512)         32833     \n",
            "_________________________________________________________________\n",
            "layer_normalization_3 (Layer (None, None, 512)         1024      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 512)               2099200   \n",
            "=================================================================\n",
            "Total params: 5,933,186\n",
            "Trainable params: 5,933,186\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzsG7zK-gk4Z"
      },
      "source": [
        "combined = concatenate([image_model.output, language_model.output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alJ5UiRJgoOd"
      },
      "source": [
        "model = Dense(2304, activation = 'relu')(combined)\n",
        "model = tf.keras.layers.LayerNormalization() (model)\n",
        "#model = Activation('tanh')(model)\n",
        "model = tf.keras.layers.Dropout(0.8)(model)\n",
        "\n",
        "model = Dense(1152, activation = 'relu')(model)\n",
        "model = tf.keras.layers.LayerNormalization() (model)\n",
        "#model = Activation('tanh')(model)\n",
        "model = tf.keras.layers.Dropout(0.8)(model)\n",
        "\n",
        "model = Dense(576, activation = 'relu')(model)\n",
        "model = tf.keras.layers.LayerNormalization() (model)\n",
        "#model = Activation('tanh')(model)\n",
        "model = tf.keras.layers.Dropout(0.8)(model)\n",
        "\n",
        "model = Dense(288, activation = 'relu')(model)\n",
        "model = tf.keras.layers.LayerNormalization() (model)\n",
        "#model = Activation('tanh')(model)\n",
        "model = tf.keras.layers.Dropout(0.8)(model)\n",
        "\n",
        "model = Dense(144, activation = 'relu')(model)\n",
        "model = tf.keras.layers.LayerNormalization() (model)\n",
        "#model = Activation('tanh')(model)\n",
        "model = tf.keras.layers.Dropout(0.8)(model)\n",
        "\n",
        "model = Dense(72, activation = 'relu')(model)\n",
        "model = tf.keras.layers.LayerNormalization() (model)\n",
        "#model = Activation('tanh')(model)\n",
        "model = tf.keras.layers.Dropout(0.8)(model)\n",
        "\n",
        "model = Dense(41)(model)\n",
        "model = Activation(\"softmax\")(model)\n",
        "\n",
        "model = tf.keras.Model(inputs=[image_model.input, language_model.input], outputs=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNkF01H_gvWl",
        "outputId": "25928f3f-6c9a-4c02-9d59-e0bc51d9d67e"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "lstm_input (InputLayer)         [(None, None, 300)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 512)    1665024     lstm_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, None, 512)    1024        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "seq_self_attention (SeqSelfAtte (None, None, 512)    32833       layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, None, 512)    1024        seq_self_attention[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, None, 512)    2099200     layer_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_2 (LayerNor (None, None, 512)    1024        lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "seq_self_attention_1 (SeqSelfAt (None, None, 512)    32833       layer_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_input (InputLayer)      [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, None, 512)    1024        seq_self_attention_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 4096)         0           reshape_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 512)          2099200     layer_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 4608)         0           reshape[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2304)         10619136    concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_10 (LayerNo (None, 2304)         4608        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 2304)         0           layer_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1152)         2655360     dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_11 (LayerNo (None, 1152)         2304        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 1152)         0           layer_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 576)          664128      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_12 (LayerNo (None, 576)          1152        dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 576)          0           layer_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 288)          166176      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_13 (LayerNo (None, 288)          576         dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 288)          0           layer_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 144)          41616       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_14 (LayerNo (None, 144)          288         dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 144)          0           layer_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 72)           10440       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_15 (LayerNo (None, 72)           144         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 72)           0           layer_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 41)           2993        dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 41)           0           dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 20,102,107\n",
            "Trainable params: 20,102,107\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee1Fhe5Ugxcg"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_questions_tensor_timeseries(questions, nlp, timesteps):\n",
        "    assert not isinstance(questions, list)\n",
        "    nb_samples = len(questions) \n",
        "    word_vec_dim = nlp(questions[0])[0].vector.shape[0] \n",
        "    questions_tensor = np.zeros((nb_samples, timesteps, word_vec_dim)) \n",
        "    for i in range(len(questions)): \n",
        "        tokens = nlp(questions[i]) \n",
        "        for j in range(len(tokens)):\n",
        "            if j<timesteps: \n",
        "                questions_tensor[i,j,:] = tokens[j].vector\n",
        "    return questions_tensor\n",
        "\n",
        "def get_images_matrix(img_coco_ids, img_map, VGGfeatures): \n",
        "    assert not isinstance(img_coco_ids, list)\n",
        "    nb_samples = len(img_coco_ids) \n",
        "    nb_dimensions = VGGfeatures.shape[0] \n",
        "    image_matrix = np.zeros((nb_samples, nb_dimensions)) \n",
        "    for j in range(len(img_coco_ids)): \n",
        "        image_matrix[j,:] = VGGfeatures[:,img_map[img_coco_ids[j]]]\n",
        "    return image_matrix\n",
        "\n",
        "def get_answers_sum(answers, encoder):\n",
        "    assert not isinstance(answers, list)\n",
        "    y = encoder.transform(answers) \n",
        "    nb_classes = encoder.classes_.shape[0] \n",
        "    Y = np_utils.to_categorical(y, nb_classes) \n",
        "    return Y\n",
        "\n",
        "def grouped(iterable, n, fillvalue=None):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(*args, fillvalue=fillvalue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ruVrjPcamS"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO8thd1QRISX",
        "outputId": "1cb5bae2-2077-450e-e67d-43f3f72ae951"
      },
      "source": [
        "batch_size = 4511\n",
        "for k in range(num_epochs):\n",
        "    print(\"Epoch Number: \",k+1)\n",
        "    progbar = generic_utils.Progbar(len(train_questions))\n",
        "    for question_batch, ans_batch, im_batch in zip(grouped(questions, batch_size, fillvalue=train_questions[-1]), \n",
        "                                               grouped(answers, batch_size, fillvalue=train_answers[-1]),\n",
        "                                               grouped(image_id, batch_size, fillvalue=train_image_id[-1])):\n",
        "        timestep = len(nlp(question_batch[-1]))\n",
        "        X_ques_batch = get_questions_tensor_timeseries(question_batch, nlp, timestep)\n",
        "        X_img_batch = get_images_matrix(im_batch, id_map, features_train)\n",
        "        Y_batch = get_answers_sum(ans_batch, le)\n",
        "        loss = model.train_on_batch(({'lstm_input' : X_ques_batch, 'reshape_input' : X_img_batch}), Y_batch)\n",
        "        progbar.add(batch_size, values=[('train loss', loss)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Number:  1\n",
            "4511/4511 [==============================] - 244s 54ms/step - train loss: 2.5364\n",
            "Epoch Number:  2\n",
            "4511/4511 [==============================] - 232s 51ms/step - train loss: 2.5253\n",
            "Epoch Number:  3\n",
            "4511/4511 [==============================] - 231s 51ms/step - train loss: 2.4994\n",
            "Epoch Number:  4\n",
            "4511/4511 [==============================] - 236s 52ms/step - train loss: 2.5096\n",
            "Epoch Number:  5\n",
            "4511/4511 [==============================] - 236s 52ms/step - train loss: 2.5062\n",
            "Epoch Number:  6\n",
            "4511/4511 [==============================] - 233s 52ms/step - train loss: 2.5094\n",
            "Epoch Number:  7\n",
            "4511/4511 [==============================] - 236s 52ms/step - train loss: 2.5055\n",
            "Epoch Number:  8\n",
            "4511/4511 [==============================] - 236s 52ms/step - train loss: 2.4965\n",
            "Epoch Number:  9\n",
            "4511/4511 [==============================] - 239s 53ms/step - train loss: 2.4851\n",
            "Epoch Number:  10\n",
            "4511/4511 [==============================] - 236s 52ms/step - train loss: 2.4820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXeQ5cpHlw6v"
      },
      "source": [
        "###Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gZaNPXdn0XH"
      },
      "source": [
        "label_encoder = pickle.load(open('/content/drive/label_encoder_lstm.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjOeeu1phwf"
      },
      "source": [
        "y_pred = []\n",
        "batch_size = 1429 \n",
        "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
        "pbar = ProgressBar(widgets=widgets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k9Yriu__NMw",
        "outputId": "e49f3c22-e18d-4b73-d603-3e2decb2aaed"
      },
      "source": [
        "id_map1 = dict()\n",
        "i = 0\n",
        "img_ids1 = open('/content/drive/Processed Text Files/Test Files/images_id.txt','rb').read().decode('utf-8').splitlines()\n",
        "for ids in np.unique(img_ids1):\n",
        "  id_map1[ids] = i\n",
        "  i+=1\n",
        "print(id_map1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'10163': 0, '10164': 1, '10167': 2, '10174': 3, '10183': 4, '10808': 5, '10812': 6, '10813': 7, '10814': 8, '10823': 9, '10829': 10, '10838': 11, '10839': 12, '10843': 13, '11483': 14, '6336': 15, '6342': 16, '6353': 17, '6362': 18, '6371': 19, '6377': 20, '6383': 21, '6389': 22, '6391': 23, '6394': 24, '6405': 25, '6412': 26, '6417': 27, '6419': 28, '6420': 29, '6445': 30, '6449': 31, '6452': 32, '6467': 33, '6468': 34, '6476': 35, '6488': 36, '6514': 37, '6517': 38, '6536': 39, '6545': 40, '6550': 41, '6553': 42, '6557': 43, '6558': 44, '6561': 45, '6562': 46, '6567': 47, '6568': 48, '6583': 49, '6590': 50, '6593': 51, '6594': 52, '6602': 53, '6619': 54, '6624': 55, '6632': 56, '6638': 57, '6659': 58, '6663': 59, '6671': 60, '6672': 61, '6677': 62, '6679': 63, '6680': 64, '6688': 65, '6691': 66, '6695': 67, '6700': 68, '6717': 69, '6718': 70, '6733': 71, '6744': 72, '6751': 73, '6761': 74, '6765': 75, '6768': 76, '6771': 77, '6774': 78, '6775': 79, '6778': 80, '6780': 81, '6785': 82, '6786': 83, '6794': 84, '6795': 85, '6796': 86, '6809': 87, '6815': 88, '6825': 89, '6827': 90, '6831': 91, '6844': 92, '6850': 93, '6856': 94, '6893': 95, '6895': 96, '6896': 97, '6898': 98, '6905': 99, '6906': 100, '6907': 101, '6910': 102, '6912': 103, '6915': 104, '6920': 105, '6927': 106, '6951': 107, '6952': 108, '6954': 109, '6959': 110, '6961': 111, '6963': 112, '6979': 113, '7004': 114, '7016': 115, '7023': 116, '7029': 117, '7031': 118, '7035': 119, '7041': 120, '7042': 121, '7054': 122, '7056': 123, '7057': 124, '7063': 125, '7069': 126, '7085': 127, '7092': 128, '7105': 129, '7126': 130, '7141': 131, '7143': 132, '7149': 133, '7154': 134, '7158': 135, '7172': 136, '7191': 137, '7213': 138, '7215': 139, '7220': 140, '7223': 141, '7231': 142, '7232': 143, '7233': 144, '7241': 145, '7253': 146, '7268': 147, '7279': 148, '7284': 149, '7291': 150, '7294': 151, '7300': 152, '7301': 153, '7307': 154, '7308': 155, '7315': 156, '7320': 157, '7323': 158, '7333': 159, '7339': 160, '7348': 161, '7355': 162, '7359': 163, '7361': 164, '7370': 165, '7376': 166, '7406': 167, '7408': 168, '7413': 169, '7415': 170, '7420': 171, '7423': 172, '7431': 173, '7444': 174, '7450': 175, '7457': 176, '7461': 177, '7464': 178, '7476': 179, '7486': 180, '7541': 181, '7543': 182, '7577': 183, '7581': 184, '7583': 185, '7584': 186, '7586': 187, '7590': 188, '7593': 189, '7595': 190, '7604': 191, '7605': 192, '7611': 193, '7616': 194, '7644': 195, '7647': 196, '7651': 197, '7652': 198, '7658': 199, '7666': 200, '7684': 201, '7689': 202, '7690': 203, '7698': 204, '7708': 205, '7723': 206, '7730': 207, '7733': 208, '7740': 209, '7743': 210, '7755': 211, '7757': 212, '7758': 213, '7759': 214, '7763': 215, '7771': 216, '7772': 217, '7773': 218, '7780': 219, '7794': 220, '7803': 221, '7804': 222, '7812': 223, '7813': 224, '7817': 225, '7821': 226, '7822': 227, '7835': 228, '7839': 229, '7840': 230, '7853': 231, '7866': 232, '7874': 233, '7876': 234, '7877': 235, '7880': 236, '7881': 237, '7888': 238, '7889': 239, '7899': 240, '7903': 241, '7904': 242, '7906': 243, '7911': 244, '7915': 245, '7921': 246, '7926': 247, '7927': 248, '7928': 249, '7930': 250, '7935': 251, '7946': 252, '7956': 253, '7957': 254, '7960': 255, '7961': 256, '7964': 257, '7969': 258, '7983': 259, '7985': 260, '7986': 261, '7993': 262, '7995': 263, '7997': 264, '8007': 265, '8017': 266, '8018': 267, '8021': 268, '8022': 269, '8026': 270, '8027': 271, '8028': 272, '8031': 273, '8040': 274, '8041': 275, '8048': 276, '8062': 277, '8064': 278, '8066': 279, '8069': 280, '8073': 281, '8076': 282, '8100': 283, '8103': 284, '8104': 285, '8111': 286, '8112': 287, '8115': 288, '8125': 289, '8126': 290, '8136': 291, '8140': 292, '8147': 293, '8154': 294, '8160': 295, '8162': 296, '8169': 297, '8170': 298, '8183': 299, '8189': 300, '8194': 301, '8195': 302, '8199': 303, '8201': 304, '8203': 305, '8211': 306, '8214': 307, '8219': 308, '8233': 309, '8237': 310, '8250': 311, '8259': 312, '8262': 313, '8279': 314, '8287': 315, '8290': 316, '8292': 317, '8293': 318, '8302': 319, '8303': 320, '8307': 321, '8308': 322, '8309': 323, '8310': 324, '8316': 325, '8324': 326, '8329': 327, '8332': 328, '8338': 329, '8339': 330, '8340': 331, '8341': 332, '8343': 333, '8346': 334, '8356': 335, '8358': 336, '8362': 337, '8363': 338, '8364': 339, '8367': 340, '8372': 341, '8376': 342, '8378': 343, '8391': 344, '8397': 345, '8399': 346, '8401': 347, '8402': 348, '8421': 349, '8428': 350, '8439': 351, '8440': 352, '8447': 353, '8462': 354, '8478': 355, '8486': 356, '8494': 357, '8511': 358, '8514': 359, '8523': 360, '8525': 361, '8530': 362, '8533': 363, '8541': 364, '8542': 365, '8543': 366, '8544': 367, '8557': 368, '8558': 369, '8592': 370, '8601': 371, '8762': 372, '8767': 373, '8773': 374, '8776': 375, '8778': 376, '8781': 377, '8784': 378, '8789': 379, '8792': 380, '8795': 381, '8797': 382, '8810': 383, '8812': 384, '8814': 385, '8815': 386, '8860': 387, '8898': 388, '8902': 389, '8912': 390, '8919': 391, '8926': 392, '8930': 393, '8934': 394, '8937': 395, '8942': 396, '8945': 397, '8948': 398, '8949': 399, '8959': 400, '8961': 401, '8972': 402, '8981': 403, '8983': 404, '8989': 405, '8991': 406, '8993': 407, '8999': 408, '9001': 409, '9002': 410, '9004': 411, '9006': 412, '9009': 413, '9012': 414, '9013': 415, '9018': 416, '9032': 417, '9035': 418, '9037': 419, '9042': 420, '9044': 421, '9045': 422, '9054': 423, '9055': 424, '9056': 425, '9058': 426, '9060': 427, '9067': 428, '9068': 429, '9082': 430, '9092': 431, '9096': 432, '9107': 433, '9109': 434, '9112': 435, '9113': 436, '9114': 437, '9231': 438, '9344': 439, '9348': 440, '9367': 441, '9539': 442, '9721': 443, '9872': 444, '9878': 445, '9883': 446, '9886': 447, '9889': 448, '9891': 449}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NauUqXYSs_JZ"
      },
      "source": [
        "test_questions = open('/content/drive/Processed Text Files/Test Files/ques.txt', 'rb').read().decode('utf-8').splitlines()\n",
        "test_image_id = open('/content/drive/Processed Text Files/Test Files/images_id.txt', 'rb').read().decode('utf-8').splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuCplzu7pnyy",
        "outputId": "6cf004e8-ff5c-4741-e6eb-8b9f5081ceba"
      },
      "source": [
        "for qu_batch,im_batch in pbar(zip(grouped(test_questions, batch_size , \n",
        "                                                   fillvalue=test_questions[0]), \n",
        "                                           grouped(test_image_id, batch_size, \n",
        "                                                   fillvalue=test_image_id[0]))):\n",
        "    timesteps = len(nlp(qu_batch[-1]))\n",
        "    X_ques_batch_test = get_questions_tensor_timeseries(qu_batch, nlp, timesteps)\n",
        "    X_img_batch_test = get_images_matrix(im_batch, id_map1, features_test)\n",
        "    y_predict = model_.predict(({'lstm_input' : X_ques_batch_test, 'reshape_input' : X_img_batch_test}))\n",
        "    y_predict = np.argmax(y_predict,axis=1)\n",
        "    y_predict = label_encoder.inverse_transform(y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating N/A% [#                                             ] Time:  0:15:14"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3rCbBC2dDZ1",
        "outputId": "b05c9ec1-cf3e-40d6-ae04-128c81e51612"
      },
      "source": [
        "#This was used to check \n",
        "#if the number of questions==number of predictions\n",
        "print(y_pred)\n",
        "print(len(y_predict))\n",
        "print(len(test_questions))\n",
        "print(len(test_image_id))\n",
        "print(features_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "1429\n",
            "1429\n",
            "1429\n",
            "(4096, 450)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcAYjUmjSBeN",
        "outputId": "c88927ca-2fed-4e43-844c-4e8524ce9126"
      },
      "source": [
        "print(y_predict[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "non flooded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd_QKG40sl8J"
      },
      "source": [
        "f= open(\"answer.txt\",\"w+\")\n",
        "for i in y_predict:\n",
        "  f.write(i+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}